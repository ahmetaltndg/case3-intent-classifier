# EXPERIMENTS.md - Deney GÃ¼nlÃ¼ÄŸÃ¼ ve SonuÃ§larÄ±

## ğŸ§ª Deney Genel BakÄ±ÅŸ

Bu dokÃ¼man, CASE 3 Intent Classifier'Ä±n geliÅŸtirilmesi sÄ±rasÄ±nda yapÄ±lan tÃ¼m deneyleri takip eder, data generation, model training ve evaluation aÅŸamalarÄ±nÄ± iÃ§erir.

## ğŸ“Š Veri Ãœretimi Deneyleri

### Deney 1: LLM SeÃ§imi
**Tarih**: 2025-09-24  
**AmaÃ§**: Synthetic data generation iÃ§in optimal LLM seÃ§imi

**Test Edilen Adaylar**:
- OpenAI GPT-4: YÃ¼ksek kalite, pahalÄ±
- OpenAI GPT-3.5-turbo: Ä°yi kalite, orta maliyet
- Google Gemini 2.0 Flash: Ä°yi kalite, maliyet-etkin

**SonuÃ§lar**:
```
Model                | Kalite | Maliyet | Ã‡ok Dilli | Karar
---------------------|--------|---------|-----------|----------
GPT-4               | 9/10   | YÃ¼ksek  | 9/10      | âŒ Ã‡ok pahalÄ±
GPT-3.5-turbo       | 7/10   | Orta    | 7/10      | âŒ Quota aÅŸÄ±ldÄ±
Gemini 2.0 Flash    | 8/10   | DÃ¼ÅŸÃ¼k   | 8/10      | âœ… SeÃ§ildi
```

**SonuÃ§**: Maliyet-etkinlik ve iyi Ã§ok dilli destek iÃ§in Gemini 2.0 Flash seÃ§ildi.

### Deney 2: Veri Hacmi Optimizasyonu
**Tarih**: 2025-09-25  
**AmaÃ§**: Intent baÅŸÄ±na optimal Ã¶rnek sayÄ±sÄ±nÄ± belirleme

**Test Edilen Hacimler**:
- 100 Ã¶rnek/intent (baseline gereksinim)
- 120 Ã¶rnek/intent (deduplication iÃ§in hedef)
- 150 Ã¶rnek/intent (aÅŸÄ±rÄ±)

**SonuÃ§lar**:
```
Hacim | Ãœretilen | Dedup SonrasÄ± | Kalite | Karar
------|----------|---------------|--------|----------
100   | 1,200    | ~900          | Ä°yi    | âŒ Yetersiz
120   | 1,440    | ~1,100        | Ä°yi    | âœ… Optimal
150   | 1,800    | ~1,400        | Ä°yi    | âŒ AÅŸÄ±rÄ±
```

**SonuÃ§**: Intent baÅŸÄ±na 120 Ã¶rnek optimal denge saÄŸlar.

### Deney 3: Deduplication Stratejisi
**Tarih**: 2025-09-26  
**AmaÃ§**: En iyi deduplication yÃ¶ntemini seÃ§me

**Test Edilen YÃ¶ntemler**:
- MinHash: HÄ±zlÄ±, yaklaÅŸÄ±k
- Embedding + Clustering: DoÄŸru, yavaÅŸ
- Simple string matching: HÄ±zlÄ±, sÄ±nÄ±rlÄ±

**SonuÃ§lar**:
```
YÃ¶ntem                | DoÄŸruluk | HÄ±z   | Memory | Karar
----------------------|----------|-------|--------|----------
MinHash              | 7/10     | 9/10  | 8/10   | âŒ Ã‡ok yaklaÅŸÄ±k
Embedding + DBSCAN   | 9/10     | 6/10  | 6/10   | âœ… SeÃ§ildi
String Matching      | 5/10     | 10/10 | 10/10  | âŒ Ã‡ok sÄ±nÄ±rlÄ±
```

**Final Dataset**: 12 intent boyunca 2,713 Ã¶rnek

## ğŸ¤– Model EÄŸitimi Deneyleri

### Deney 4: Baseline Model KonfigÃ¼rasyonu
**Tarih**: 2025-09-27  
**AmaÃ§**: TF-IDF + Logistic Regression optimizasyonu

**Test Edilen Parametreler**:
```python
# KonfigÃ¼rasyon A: Conservative
max_features = 5,000
ngram_range = (1, 2)
C = 1.0

# KonfigÃ¼rasyon B: Balanced (SeÃ§ilen)
max_features = 10,000
ngram_range = (1, 2)
C = 1.0

# KonfigÃ¼rasyon C: Aggressive
max_features = 20,000
ngram_range = (1, 3)
C = 10.0
```

**SonuÃ§lar**:
```
Config | DoÄŸruluk | F1-Score | HÄ±z   | Memory | Karar
-------|----------|----------|-------|--------|----------
A      | 82.1%    | 0.815    | 9/10  | 9/10   | âŒ Ã‡ok conservative
B      | 86.5%    | 0.861    | 8/10  | 7/10   | âœ… SeÃ§ildi
C      | 87.2%    | 0.868    | 6/10  | 5/10   | âŒ Ã‡ok kaynak yoÄŸun
```

### Deney 5: Transformer Model Fine-tuning
**Tarih**: 2025-09-28  
**AmaÃ§**: DistilBERT fine-tuning optimizasyonu

**Test Edilen Hyperparametreler**:
```python
# KonfigÃ¼rasyon A: Conservative
learning_rate = 2e-5
epochs = 2
batch_size = 16

# KonfigÃ¼rasyon B: Balanced (SeÃ§ilen)
learning_rate = 5e-5
epochs = 3
batch_size = 8

# KonfigÃ¼rasyon C: Aggressive
learning_rate = 1e-4
epochs = 5
batch_size = 4
```

**EÄŸitim Ä°lerlemesi**:
```
Epoch | Train Loss | Eval Loss | DoÄŸruluk
------|------------|-----------|----------
1     | 2.48       | 1.19      | 74.0%
2     | 0.78       | 0.42      | 87.0%
3     | 0.15       | 0.27      | 93.0%
```

**SonuÃ§lar**:
```
Config | DoÄŸruluk | F1-Score | EÄŸitim SÃ¼resi | Karar
-------|----------|----------|---------------|----------
A      | 89.1%    | 0.891    | 15 dk        | âŒ Underfitted
B      | 93.0%    | 0.930    | 42 dk        | âœ… SeÃ§ildi
C      | 92.8%    | 0.928    | 90 dk        | âŒ Overfitted
```

## ğŸ“ˆ Kalibrasyon Deneyleri

### Deney 6: Kalibrasyon YÃ¶ntemi KarÅŸÄ±laÅŸtÄ±rmasÄ±
**Tarih**: 2025-09-28  
**AmaÃ§**: Kalibrasyon tekniklerini karÅŸÄ±laÅŸtÄ±rma

**Test Edilen YÃ¶ntemler**:
- No Calibration (baseline)
- Platt Scaling (logistic regression)
- Temperature Scaling (neural networks)
- Isotonic Regression

**SonuÃ§lar**:
```
Model       | YÃ¶ntem           | Brier Score | ECE    | Karar
------------|------------------|-------------|--------|----------
Baseline    | No Calibration   | 0.623       | 0.089  | âŒ KÃ¶tÃ¼
Baseline    | Platt Scaling    | 0.577       | 0.045  | âœ… SeÃ§ildi
Transformer | No Calibration   | 0.945       | 0.156  | âŒ KÃ¶tÃ¼
Transformer | Temperature      | 0.915       | 0.078  | âœ… SeÃ§ildi
```

### Deney 7: Confidence Threshold Optimizasyonu
**Tarih**: 2025-09-28  
**AmaÃ§**: Optimal rejection threshold bulma

**Test Edilen Threshold'lar**: 0.5, 0.6, 0.7, 0.8, 0.9

**SonuÃ§lar**:
```
Threshold | Precision | Recall | Abstain OranÄ± | Karar
----------|-----------|--------|---------------|----------
0.5       | 0.78      | 0.95   | 5%            | âŒ Ã‡ok permissive
0.6       | 0.82      | 0.92   | 8%            | âŒ Hala dÃ¼ÅŸÃ¼k precision
0.7       | 0.87      | 0.88   | 12%           | âœ… SeÃ§ildi
0.8       | 0.92      | 0.81   | 19%           | âŒ Ã‡ok restrictive
0.9       | 0.96      | 0.72   | 28%           | âŒ Ã‡ok fazla abstention
```

## ğŸŒ Cross-lingual Deneyleri

### Deney 8: Dil Performans Analizi
**Tarih**: 2025-09-28  
**AmaÃ§**: TR vs EN performansÄ±nÄ± deÄŸerlendirme

**Test Seti**: 20 Ã¶rnek (12 TÃ¼rkÃ§e, 8 Ä°ngilizce)

**SonuÃ§lar**:
```
Model       | Dil      | DoÄŸruluk | F1-Score | Notlar
------------|----------|----------|----------|-------
Baseline    | TÃ¼rkÃ§e   | 75.0%    | 0.67     | DÃ¼ÅŸÃ¼k performans
Baseline    | Ä°ngilizce| 100.0%   | 1.00     | MÃ¼kemmel
Transformer | TÃ¼rkÃ§e   | 83.3%    | 0.78     | Baseline'dan daha iyi
Transformer | Ä°ngilizce| 100.0%   | 1.00     | MÃ¼kemmel
```

**Analiz**: Ä°ngilizce performans tutarlÄ± olarak daha iyi Ã§Ã¼nkÃ¼:
1. DistilBERT'te daha fazla Ä°ngilizce training data
2. TF-IDF Ä°ngilizce word boundary'lerle daha iyi Ã§alÄ±ÅŸÄ±r
3. TÃ¼rkÃ§e agglutinative doÄŸasÄ± her iki modeli de zorlar

## ğŸ”¬ Ablation Studies

### Deney 9: Feature Ablation
**Tarih**: 2025-09-28  
**AmaÃ§**: Feature Ã¶nemini anlama

**Test Edilen KonfigÃ¼rasyonlar**:
```
KonfigÃ¼rasyon           | DoÄŸruluk | F1-Score | Notlar
------------------------|----------|----------|-------
Full Model              | 86.56%   | 0.861    | Baseline
No Calibration          | 86.56%   | 0.861    | Fark yok
Limited Features (1K)   | 86.19%   | 0.849    | Hafif dÃ¼ÅŸÃ¼ÅŸ
Unigrams Only           | 85.45%   | 0.859    | Bigrams yardÄ±m eder
Trigrams                | 86.56%   | 0.872    | En iyi performans
```

**SonuÃ§**: Bigrams ve trigrams Ã¶nemli deÄŸer saÄŸlar.

### Deney 10: Veri Boyutu Ablation
**Tarih**: 2025-09-28  
**AmaÃ§**: Veri gereksinimlerini anlama

**SonuÃ§lar**:
```
Veri Boyutu | DoÄŸruluk | F1-Score | Ä°yileÅŸme
------------|----------|----------|----------
10%         | 65.01%   | 0.601    | Baseline
25%         | 75.69%   | 0.708    | +10.68%
50%         | 81.22%   | 0.842    | +5.53%
75%         | 84.90%   | 0.862    | +3.68%
100%        | 86.37%   | 0.854    | +1.47%
```

**Analiz**: %75'ten sonra diminishing returns, ama %100 hala faydalÄ±.

## ğŸ¯ Final Model PerformansÄ±

### Production Metrikleri
```
Model       | DoÄŸruluk | F1-Score | Latency | Memory
------------|----------|----------|---------|--------
Baseline    | 86.5%    | 0.861    | 3ms     | 50MB
Transformer | 93.0%    | 0.930    | 75ms    | 500MB
```

### Hata Analizi
**YaygÄ±n YanlÄ±ÅŸ SÄ±nÄ±flandÄ±rmalar**:
1. **TÃ¼rkÃ§e â†’ Ä°ngilizce**: "KÃ¼tÃ¼phane saat kaÃ§ta aÃ§Ä±lÄ±yor?" bazen Ä°ngilizce olarak sÄ±nÄ±flandÄ±rÄ±lÄ±r
2. **Intent KarÄ±ÅŸÄ±klÄ±ÄŸÄ±**: `complaint` vs `other` (belirsiz sorgular)
3. **DÃ¼ÅŸÃ¼k Confidence**: KarmaÅŸÄ±k sorgular abstain'i doÄŸru tetikler



## ğŸ”„ Deney Reproducibility

TÃ¼m deneyler ÅŸunlarla reproduce edilebilir:
- **Veri**: `scripts/generate_synthetic_data.py`
- **EÄŸitim**: `scripts/train_baseline.py`, `scripts/train_transformer.py`
- **DeÄŸerlendirme**: `scripts/generate_calibration_curves.py`, `scripts/generate_ablation_study.py`
- **KonfigÃ¼rasyon**: TÃ¼m hyperparametreler ilgili script'lerde dokÃ¼mante edilmiÅŸ

**Random Seeds**: Reproducibility iÃ§in sabit (seed=42)
**Environment**: Python 3.12, requirements.txt saÄŸlanmÄ±ÅŸ
